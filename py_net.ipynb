{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "py_net.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parth-mango/EVA5-Assignments/blob/main/py_net.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_v_6sMwK5Dp"
      },
      "source": [
        "# Steps:\r\n",
        "\r\n",
        "1. Import libraries \r\n",
        "2. Create a training set\r\n",
        "3. create a dataloader\r\n",
        "4. Define a network: <br>\r\n",
        "   a. Define layers in the __init__ function <br>\r\n",
        "   b. Make a forward function<br>\r\n",
        "   c. Define optimizer<br>\r\n",
        "   d. Call backward function to propagate the loss backwards <br>\r\n",
        "5. Train the network for defined epochs, get predictions & propagate the loss backwards <br>  \r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmAiu_SuMhEE"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import torch.optim as optim\r\n",
        "import torchvision\r\n",
        "from torchvision import transforms\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "# !pip install torchsummary\r\n",
        "from torchsummary import summary"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wl225Nt9MzK1"
      },
      "source": [
        "train_set= torchvision.datasets.EMNIST('/content/data/', split= 'byclass', train= True, download= True, transform= transforms.Compose([transforms.ToTensor()]))"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "USnAR4I77pqI",
        "outputId": "45a4a885-0074-4850-e72e-f646de80df5e"
      },
      "source": [
        "train_set.train_labels.bincount() # unbalanced dataset 'byclass' specified by us above - 62 classes"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:48: UserWarning: train_labels has been renamed targets\n",
            "  warnings.warn(\"train_labels has been renamed targets\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([34585, 38374, 34203, 35143, 33535, 31416, 34232, 35754, 33946, 33847,\n",
              "         6407,  3878, 10094,  4562,  4934,  9182,  2517,  3152, 11946,  3762,\n",
              "         2468,  5076,  9002,  8237, 24983,  8347,  2605,  5073, 20764,  9820,\n",
              "        12602,  4637,  4695,  2771,  4743,  2701, 10033,  5159,  2854, 10177,\n",
              "        24631,  2561,  3687,  8738,  2725,  1896,  2491, 15318,  2645, 11418,\n",
              "         2749,  2448,  2994, 14105,  2699, 18262,  2830,  2910,  2697,  2822,\n",
              "         2365,  2725])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZm66TyC-i8D",
        "outputId": "64338ebc-796b-497c-a9eb-050e9e511a03"
      },
      "source": [
        "sample= next(iter(train_set)) # train_set is an iterator \r\n",
        "\r\n",
        "img, y= sample\r\n",
        "\r\n",
        "print(img.shape, y)  # Priniting the image shape and label for the image"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 28, 28]) 35\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pyJwyJkU0fv",
        "outputId": "852458f4-85cd-4422-be7e-76dc02489e73"
      },
      "source": [
        "len(train_set) # That's the number of training samples"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "697932"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mOU-ApYTBMB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "4f8d5aa4-24e5-4369-c7b6-5ec3ea5db1d1"
      },
      "source": [
        "sample_img= torch.reshape(train_set[0][0], (28, 28, -1)).squeeze()\r\n",
        "plt.imshow(sample_img, cmap= 'gray') # plotting the image"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fe482ac3eb8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDXRAy5tAir9"
      },
      "source": [
        "train_dataloader= torch.utils.data.DataLoader(dataset= train_set, batch_size= 10000, shuffle= True)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpCQagxaApBa",
        "outputId": "27e35755-3b0b-48de-eca1-10e93eca5052"
      },
      "source": [
        "sample_dl= next(iter(train_dataloader))\r\n",
        "\r\n",
        "sample_dl # Outputs 10 image tensors and their labels"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([[[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "           ...,\n",
              "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
              " \n",
              " \n",
              "         [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "           ...,\n",
              "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
              " \n",
              " \n",
              "         [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "           [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],\n",
              "           [0.0000, 0.0000, 0.0000,  ..., 0.1804, 0.0000, 0.0000],\n",
              "           ...,\n",
              "           [0.0000, 0.0000, 0.1294,  ..., 0.0000, 0.0000, 0.0000],\n",
              "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
              " \n",
              " \n",
              "         ...,\n",
              " \n",
              " \n",
              "         [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "           ...,\n",
              "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
              " \n",
              " \n",
              "         [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "           ...,\n",
              "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
              " \n",
              " \n",
              "         [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "           ...,\n",
              "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]]]),\n",
              " tensor([37,  7, 35,  ..., 39,  8,  3])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0RbdcwIT510"
      },
      "source": [
        "class Net(nn.Module):\r\n",
        "  \r\n",
        "  def __init__(self):\r\n",
        "    super().__init__()\r\n",
        "    self.conv1= nn.Conv2d(in_channels= 1, out_channels= 10, kernel_size= 3, padding= 1) #28x28x1 > 28x28x10; RF: 3x3\r\n",
        "    self.conv2= nn.Conv2d(in_channels= 10, out_channels= 10, kernel_size= 3,padding= 1 ) #28x28x10 > 28x28x10; RF: 5x5\r\n",
        "    self.max_pool1= nn.MaxPool2d(2, stride= 2) #28x28x10 > 14x14x10 ; RF: 10x10\r\n",
        "    self.conv3= nn.Conv2d(in_channels= 10, out_channels= 20, kernel_size= 3, padding=1 ) #14x14x10 > 14x14x20; RF: 12x12\r\n",
        "    self.conv4= nn.Conv2d(in_channels= 20, out_channels= 20, kernel_size= 3, padding= 1) #14x14x20 > 14x14x20; RF: 14x14\r\n",
        "    self.max_pool2= nn.MaxPool2d(2, stride= 2) #14x14x20 > 7x7x20 ; RF: 28x28\r\n",
        "    self.conv5= nn.Conv2d(in_channels= 20, out_channels=30, kernel_size= 3 ) #7x7x20 > 5x5x30; RF: 30x30\r\n",
        "    self.conv6= nn.Conv2d(in_channels= 30, out_channels= 62, kernel_size= 3) #5x5x30 > 3x3x62; RF: 32x32\r\n",
        "    self.avg_pool= nn.AvgPool2d(3, stride= 2) # 3x3x30 > 1x1x62; RF: 96x96\r\n",
        "\r\n",
        "  def forward(self, t):\r\n",
        "    t= self.max_pool1(F.relu(self.conv2(F.relu(self.conv1(t)))))\r\n",
        "    t= self.max_pool2(F.relu(self.conv4(F.relu(self.conv3(t)))))\r\n",
        "    t= self.avg_pool(self.conv6(F.relu(self.conv5(t)))) # No relu in the last convolution layer\r\n",
        "    # t= t.squeeze()\r\n",
        "    t= t.view(-1, 62) # reshaping the tensor \r\n",
        "    # print(t.shape)\r\n",
        "\r\n",
        "    return F.log_softmax(t, dim= 1) #Applying softmax activation for final prediction\r\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xw-3YPildt6u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e01f10cf-cff1-484c-c620-6300dfe744fd"
      },
      "source": [
        "use_cuda = torch.cuda.is_available()\r\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\r\n",
        "\r\n",
        "print(device) #Testing is the device is GPU or CPU\r\n",
        "\r\n",
        "model = Net().to(device)\r\n",
        "\r\n",
        "summary(model, input_size=(1, 28, 28))\r\n",
        "# print(model)\r\n",
        "# print(model.conv1.weight)\r\n",
        "\r\n",
        "for params in model.parameters():\r\n",
        "  print(params.shape) # We see the output at the first place and input in the second place \r\n",
        "  \r\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 10, 28, 28]             100\n",
            "            Conv2d-2           [-1, 10, 28, 28]             910\n",
            "         MaxPool2d-3           [-1, 10, 14, 14]               0\n",
            "            Conv2d-4           [-1, 20, 14, 14]           1,820\n",
            "            Conv2d-5           [-1, 20, 14, 14]           3,620\n",
            "         MaxPool2d-6             [-1, 20, 7, 7]               0\n",
            "            Conv2d-7             [-1, 30, 5, 5]           5,430\n",
            "            Conv2d-8             [-1, 62, 3, 3]          16,802\n",
            "         AvgPool2d-9             [-1, 62, 1, 1]               0\n",
            "================================================================\n",
            "Total params: 28,682\n",
            "Trainable params: 28,682\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.21\n",
            "Params size (MB): 0.11\n",
            "Estimated Total Size (MB): 0.32\n",
            "----------------------------------------------------------------\n",
            "torch.Size([10, 1, 3, 3])\n",
            "torch.Size([10])\n",
            "torch.Size([10, 10, 3, 3])\n",
            "torch.Size([10])\n",
            "torch.Size([20, 10, 3, 3])\n",
            "torch.Size([20])\n",
            "torch.Size([20, 20, 3, 3])\n",
            "torch.Size([20])\n",
            "torch.Size([30, 20, 3, 3])\n",
            "torch.Size([30])\n",
            "torch.Size([62, 30, 3, 3])\n",
            "torch.Size([62])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3DdtlwYb7iNP",
        "outputId": "1a771e1e-aec1-4b26-d2eb-6b5392d945c0"
      },
      "source": [
        "optim= torch.optim.Adam(model.parameters(), lr= 0.001) \r\n",
        "torch.set_grad_enabled(True) #Enabling Gradient \r\n",
        "\r\n",
        "def num_correct_preds(preds, labels): # Function to get the number of correct predictions\r\n",
        "  return preds.argmax(dim= 1).eq(labels).sum().item() \r\n",
        "\r\n",
        "\r\n",
        "for epochs in range(20):\r\n",
        "  \r\n",
        "  \r\n",
        "  total_loss= 0\r\n",
        "  correct_preds= 0\r\n",
        "  total= 0\r\n",
        "\r\n",
        "  for mini_batch in train_dataloader: \r\n",
        "    images, labels= mini_batch  # Batch of images and labels\r\n",
        "    images, labels= images.to(device), labels.to(device)  \r\n",
        "    # labels=  torch.nn.functional.one_hot(labels)\r\n",
        "    # reverted = torch.argmax(one_hot, dim=1)\r\n",
        "    # assert (one_hot == reverted).all().item()\r\n",
        "\r\n",
        "    # labels = torch.tensor(labels)\r\n",
        "    # one_hot = torch.zeros(len(labels), 62)\r\n",
        "    # one_hot[torch.arange(len(labels)), labels] = 1\r\n",
        "    # reverted = torch.argmax(one_hot, dim=1)\r\n",
        "    # assert (labels == reverted).all().item()\r\n",
        "    # one_hot= one_hot.type(torch.LongTensor)\r\n",
        "    # images, labels_= images.to(device), one_hot.to(device)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "    \r\n",
        "    \r\n",
        "    preds= model(images) # Predicting through model\r\n",
        "    # print(preds)\r\n",
        "    # print(labels)\r\n",
        "\r\n",
        "    loss= F.cross_entropy(preds, labels) # calculating Cross entropy loss\r\n",
        "    # print(loss)\r\n",
        "\r\n",
        "    optim.zero_grad() # making gradient zero to prevent gradient accumulation\r\n",
        "\r\n",
        "    loss.backward()  # Backward propagation of loss\r\n",
        "\r\n",
        "    optim.step()  # Updating the weights\r\n",
        "\r\n",
        "    total_loss+= loss.item()\r\n",
        "    correct_preds+= num_correct_preds(preds, labels)\r\n",
        "    total+= labels.size(0)\r\n",
        "  \r\n",
        "  print(' Current Epoch: {}, \\n Total Correct Predictions: {}, \\n Total Loss:{} , \\n Accuracy: {} '.format(epochs, correct_preds, total_loss, correct_preds/total))\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Current Epoch: 0, \n",
            " Total Correct Predictions: 98509, \n",
            " Total Loss:250.1446406841278 , \n",
            " Accuracy: 0.1411441229231501 \n",
            " Current Epoch: 1, \n",
            " Total Correct Predictions: 372558, \n",
            " Total Loss:121.33733034133911 , \n",
            " Accuracy: 0.5338027200357628 \n",
            " Current Epoch: 2, \n",
            " Total Correct Predictions: 497350, \n",
            " Total Loss:66.92763602733612 , \n",
            " Accuracy: 0.7126052394789177 \n",
            " Current Epoch: 3, \n",
            " Total Correct Predictions: 531842, \n",
            " Total Loss:52.8263258934021 , \n",
            " Accuracy: 0.7620255268421565 \n",
            " Current Epoch: 4, \n",
            " Total Correct Predictions: 546123, \n",
            " Total Loss:47.473343312740326 , \n",
            " Accuracy: 0.7824874056498341 \n",
            " Current Epoch: 5, \n",
            " Total Correct Predictions: 554803, \n",
            " Total Loss:44.19183272123337 , \n",
            " Accuracy: 0.7949241473381361 \n",
            " Current Epoch: 6, \n",
            " Total Correct Predictions: 560571, \n",
            " Total Loss:41.87605261802673 , \n",
            " Accuracy: 0.8031885627826206 \n",
            " Current Epoch: 7, \n",
            " Total Correct Predictions: 565207, \n",
            " Total Loss:40.28536856174469 , \n",
            " Accuracy: 0.8098310437119949 \n",
            " Current Epoch: 8, \n",
            " Total Correct Predictions: 568175, \n",
            " Total Loss:38.95103162527084 , \n",
            " Accuracy: 0.8140836069989627 \n",
            " Current Epoch: 9, \n",
            " Total Correct Predictions: 571347, \n",
            " Total Loss:37.84617179632187 , \n",
            " Accuracy: 0.8186284623716924 \n",
            " Current Epoch: 10, \n",
            " Total Correct Predictions: 574117, \n",
            " Total Loss:36.82971382141113 , \n",
            " Accuracy: 0.82259733039895 \n",
            " Current Epoch: 11, \n",
            " Total Correct Predictions: 576169, \n",
            " Total Loss:36.01252210140228 , \n",
            " Accuracy: 0.8255374449086731 \n",
            " Current Epoch: 12, \n",
            " Total Correct Predictions: 578261, \n",
            " Total Loss:35.266690492630005 , \n",
            " Accuracy: 0.8285348715920748 \n",
            " Current Epoch: 13, \n",
            " Total Correct Predictions: 579575, \n",
            " Total Loss:34.76111423969269 , \n",
            " Accuracy: 0.8304175764974239 \n",
            " Current Epoch: 14, \n",
            " Total Correct Predictions: 581214, \n",
            " Total Loss:34.13481080532074 , \n",
            " Accuracy: 0.8327659428139131 \n",
            " Current Epoch: 15, \n",
            " Total Correct Predictions: 582582, \n",
            " Total Loss:33.674726724624634 , \n",
            " Accuracy: 0.8347260191537285 \n",
            " Current Epoch: 16, \n",
            " Total Correct Predictions: 583762, \n",
            " Total Loss:33.18701633810997 , \n",
            " Accuracy: 0.8364167282772533 \n",
            " Current Epoch: 17, \n",
            " Total Correct Predictions: 584737, \n",
            " Total Loss:32.850173115730286 , \n",
            " Accuracy: 0.8378137125106744 \n",
            " Current Epoch: 18, \n",
            " Total Correct Predictions: 586084, \n",
            " Total Loss:32.37184923887253 , \n",
            " Accuracy: 0.8397436999593083 \n",
            " Current Epoch: 19, \n",
            " Total Correct Predictions: 586902, \n",
            " Total Loss:32.08806136250496 , \n",
            " Accuracy: 0.84091573391104 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngjXwps9EtPt"
      },
      "source": [
        ""
      ],
      "execution_count": 11,
      "outputs": []
    }
  ]
}